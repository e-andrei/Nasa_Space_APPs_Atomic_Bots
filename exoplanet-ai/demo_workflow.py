#!/usr/bin/env python3
"""
Quick demonstration of the complete ML workflow including TOI data analysis
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from model import train_from_csv, ExoplanetClassifier, load_dataset, train_from_multiple_csv

def demo_workflow():
    print("ğŸš€ Quick ML Workflow Demonstration - Model Unificat")
    print("=" * 50)
    
    # Step 1: Train unified model on all datasets
    print("Step 1: Antrenarea modelului unificat pe toate datele...")
    
    # Lista tuturor seturilor de date
    all_datasets = [
        "date/cumulative_2025.10.03_23.13.19.csv",  # Kepler
        "date/k2pandc_2025.10.03_23.45.46.csv",     # K2
        "date/TOI_2025.10.04_00.04.19.csv"          # TESS TOI
    ]
    
    # VerificÄƒ ce seturi sunt disponibile
    available_datasets = [f for f in all_datasets if Path(f).exists()]
    print(f"Seturi de date disponibile: {len(available_datasets)}")
    for dataset in available_datasets:
        print(f"  âœ… {Path(dataset).name}")
    
    if len(available_datasets) >= 2:
        # AntreneazÄƒ model pe toate datele simultan
        try:
            result = train_from_multiple_csv(
                data_paths=available_datasets,
                output_model_path="models/demo_unified_model.joblib",
                model_type='xgboost',
                tune=False,  # Skip tuning for demo speed
                random_state=42
            )
            
            print(f"âœ… Model unificat antrenat cu succes!")
            print(f"   Accuracy: {result['metadata']['eval_metrics']['accuracy']:.3f}")
            print(f"   Macro F1: {result['metadata']['eval_metrics']['macro_f1']:.3f}")
            print(f"   ProceseazÄƒ {len(available_datasets)} formate simultane")
            
            # ÃncarcÄƒ modelul pentru analizÄƒ
            clf = ExoplanetClassifier.load("models/demo_unified_model.joblib")
            
        except Exception as e:
            print(f"   âš ï¸  Eroare la antrenarea modelului unificat: {str(e)}")
            print("   Revin la modelul simplu...")
            # Fallback la modelul simplu
            result = train_from_csv(
                data_path="date/cumulative_2025.10.03_23.13.19.csv",
                output_model_path="models/demo_model.joblib",
                model_type='random_forest',
                tune=False,
                random_state=42
            )
            clf = ExoplanetClassifier.load("models/demo_model.joblib")
    else:
        print("   âš ï¸  Nu sunt suficiente seturi de date pentru model unificat")
        result = train_from_csv(
            data_path="date/cumulative_2025.10.03_23.13.19.csv",
            output_model_path="models/demo_model.joblib",
            model_type='random_forest',
            tune=False,
            random_state=42
        )
        clf = ExoplanetClassifier.load("models/demo_model.joblib")
    
    # Step 2: Test predictions
    print("\nStep 2: Testarea predicÈ›iilor pe date multiple...")
    X_test, y_test, numeric_cols, categorical_cols = load_dataset("date/cumulative_2025.10.03_23.13.19.csv")
    X_test = X_test.head(5)
    y_test = y_test.head(5)
    
    predictions = clf.predict(X_test)
    probabilities = clf.predict_proba(X_test)
    
    print("âœ… Predictions completed:")
    for i in range(len(predictions)):
        pred = predictions[i]
        conf = probabilities[i].max()
        actual = y_test.iloc[i]
        print(f"   Sample {i+1}: Predicted={pred}, Confidence={conf:.3f}, Actual={actual}")
    
    # Step 3: Comprehensive analysis of all datasets
    print("\n" + "=" * 60)
    print("Step 3: AnalizÄƒ comparativÄƒ pe toate seturile de date")
    print("=" * 60)
    
    # AnalizeazÄƒ toate seturile de date disponibile
    total_analysis = analyze_all_datasets_unified(available_datasets, clf)
    
    # Step 4: Summary and tuning recommendations
    print("\n" + "=" * 50)
    print("Step 4: Rezumat È™i recomandÄƒri de tuning")
    print("=" * 50)
    
    metrics = clf.evaluate(X_test, y_test)
    print(f"âœ… Test Accuracy: {metrics['accuracy']:.3f}")
    print(f"   Test Macro F1: {metrics['macro_f1']:.3f}")
    
    if total_analysis:
        print(f"\nğŸ“Š ANALIZA GLOBALÄ‚ MULTI-DATASET:")
        print(f"   Total obiecte analizate: {total_analysis['total_objects']:,}")
        print(f"   Accuracy medie: {total_analysis['avg_accuracy']:.3f}")
        
        print(f"\nğŸ¯ RECOMANDÄ‚RI DE ÃMBUNÄ‚TÄ‚ÈšIRE:")
        for recommendation in total_analysis['recommendations']:
            print(f"   â€¢ {recommendation}")
    
    print("\nğŸ‰ Workflow demonstration completed successfully!")
    print("\nFuncÈ›ionalitÄƒÈ›i disponibile Ã®n aplicaÈ›ia web:")
    print("1. Model unificat care proceseazÄƒ Kepler, K2 È™i TESS TOI")
    print("2. AnalizÄƒ comparativÄƒ real vs prezis pe toate seturile")
    print("3. RecomandÄƒri automate de tuning")
    print("4. Suport pentru antrenare pe date multiple simultan")
    print("5. Optimizare automatÄƒ a parametrilor")
    
    # Step 3: Analyze TOI data (TESS Objects of Interest)
    print("\n" + "=" * 60)
    print("Step 3: Analiza detaliatÄƒ a datelor TOI (TESS Objects of Interest)")
    print("=" * 60)
    
    try:
        # Load TOI data
        toi_data = pd.read_csv("date/TOI_2025.10.04_00.04.19.csv", comment='#')
        
        print(f"\nğŸ“Š Statistici generale pentru datele TOI:")
        print(f"   NumÄƒrul total de obiecte TESS: {len(toi_data):,}")
        
        # Analyze actual classifications
        if 'tfopwg_disp' in toi_data.columns:
            classification_counts = toi_data['tfopwg_disp'].value_counts()
            
            print(f"\nğŸ” DistribuÈ›ia actualÄƒ a clasificÄƒrilor Ã®n datele TOI:")
            total = len(toi_data)
            for category, count in classification_counts.items():
                percentage = (count / total * 100)
                description = get_toi_category_description(category)
                print(f"   {category} ({description}): {count:,} ({percentage:.1f}%)")
            
            # Map to standardized categories
            standardized_counts = map_toi_to_standard_categories(toi_data)
            print(f"\nğŸ“ˆ Maparea la categoriile standardizate ale modelului:")
            total_mapped = sum(standardized_counts.values())
            for category, count in standardized_counts.items():
                percentage = (count / total_mapped * 100)
                print(f"   {category}: {count:,} ({percentage:.1f}%)")
            
            print(f"\nğŸ’¡ ComparaÈ›ia distribuÈ›iilor:")
            print(f"   Datele TOI conÈ›in {standardized_counts.get('CANDIDATE', 0):,} candidaÈ›i de planete")
            print(f"   Versus {standardized_counts.get('CONFIRMED', 0):,} planete confirmate")
            print(f"   È˜i {standardized_counts.get('FALSE POSITIVE', 0):,} false pozitive")
            
            # Calculate ratios
            candidates = standardized_counts.get('CANDIDATE', 0)
            confirmed = standardized_counts.get('CONFIRMED', 0)
            if confirmed > 0:
                ratio = candidates / confirmed
                print(f"   Raportul candidaÈ›i/confirmaÈ›i: {ratio:.1f}:1")
        
        print(f"\nğŸ¯ InformaÈ›ii despre potenÈ›ialul de descoperire:")
        print(f"   â€¢ TOI-urile sunt obiecte de interes identificate de TESS")
        print(f"   â€¢ Multe dintre 'candidaÈ›i' pot fi planete reale Ã®n aÈ™teptarea confirmÄƒrii")
        print(f"   â€¢ Modelul poate ajuta la prioritizarea obiectelor pentru urmÄƒrire")
        
    except Exception as e:
        print(f"   âŒ Eroare la Ã®ncÄƒrcarea datelor TOI: {str(e)}")
    
    # Step 4: Model evaluation summary
    print("\n" + "=" * 50)
    print("Step 4: Rezumatul evaluÄƒrii modelului")
    print("=" * 50)
    
    metrics = clf.evaluate(X_test, y_test)
    print(f"âœ… Test Macro F1: {metrics['macro_f1']:.3f}")
    print(f"   Test Accuracy: {metrics['accuracy']:.3f}")
    
    print("\nğŸ‰ Workflow demonstration completed successfully!")
    print("\nFuncÈ›ionalitÄƒÈ›i disponibile Ã®n aplicaÈ›ia web:")
    print("1. ÃncÄƒrcarea È™i testarea cu fiÈ™iere CSV multiple")
    print("2. Ajustarea parametrilor modelului È™i re-antrenarea")
    print("3. Analiza importanÈ›ei caracteristicilor")
    print("4. DescÄƒrcarea rezultatelor È™i vizualizarea performanÈ›ei")
    print("5. Suport pentru formate Kepler, K2/PANDC È™i TESS TOI")

def analyze_all_datasets_unified(datasets, clf):
    """AnalizeazÄƒ toate seturile de date cu modelul unificat"""
    try:
        from model import load_dataset
        
        total_objects = 0
        total_correct = 0
        dataset_results = {}
        recommendations = []
        
        print(f"\nğŸ“Š Analizez {len(datasets)} seturi de date cu modelul unificat...")
        
        for dataset_path in datasets:
            dataset_name = Path(dataset_path).stem
            
            try:
                # ÃncarcÄƒ datele
                X, y, _, _ = load_dataset(dataset_path)
                
                # FÄƒ predicÈ›ii (doar pe primele 1000 pentru vitezÄƒ)
                sample_size = min(1000, len(X))
                X_sample = X.head(sample_size)
                y_sample = y.head(sample_size)
                
                predictions = clf.predict(X_sample)
                
                # CalculeazÄƒ metrici
                accuracy = (y_sample == predictions).sum() / len(y_sample)
                total_objects += sample_size
                total_correct += (y_sample == predictions).sum()
                
                # AnalizeazÄƒ distribuÈ›iile
                real_dist = y_sample.value_counts().to_dict()
                pred_dist = pd.Series(predictions).value_counts().to_dict()
                
                dataset_results[dataset_name] = {
                    'accuracy': accuracy,
                    'real_dist': real_dist,
                    'pred_dist': pred_dist,
                    'sample_size': sample_size
                }
                
                print(f"   ğŸ“ˆ {dataset_name}: Accuracy = {accuracy:.3f} (pe {sample_size} eÈ™antioane)")
                
                # VerificÄƒ bias-uri significative
                for category in real_dist.keys():
                    real_count = real_dist[category]
                    pred_count = pred_dist.get(category, 0)
                    bias = (pred_count - real_count) / real_count if real_count > 0 else 0
                    
                    if abs(bias) > 0.15:  # Bias mai mare de 15%
                        bias_type = "supraevalueazÄƒ" if bias > 0 else "subevalueazÄƒ"
                        recommendations.append(f"Pe {dataset_name}: {bias_type} {category} cu {abs(bias)*100:.1f}%")
                
            except Exception as e:
                print(f"   âŒ Eroare la analizarea {dataset_name}: {str(e)}")
        
        avg_accuracy = total_correct / total_objects if total_objects > 0 else 0
        
        # AdaugÄƒ recomandÄƒri generale
        if avg_accuracy < 0.9:
            recommendations.append("Accuracy globalÄƒ sub 90% - considerÄƒ tuning mai agresiv")
        if len(recommendations) == 0:
            recommendations.append("Modelul funcÈ›ioneazÄƒ bine pe toate seturile de date")
        
        return {
            'total_objects': total_objects,
            'avg_accuracy': avg_accuracy,
            'dataset_results': dataset_results,
            'recommendations': recommendations
        }
        
    except Exception as e:
        print(f"   âŒ Eroare Ã®n analiza unificatÄƒ: {str(e)}")
        return None

def get_toi_category_description(category):
    """ReturneazÄƒ descrierea completÄƒ pentru categoriile TOI"""
    descriptions = {
        'CP': 'Confirmed Planet',
        'FP': 'False Positive', 
        'PC': 'Planet Candidate',
        'KP': 'Known Planet',
        'FA': 'False Alarm',
        'APC': 'Ambiguous Planet Candidate'
    }
    return descriptions.get(category, 'Unknown')

def get_standard_category_mapping():
    """ReturneazÄƒ maparea de la categoriile TOI la categoriile standard"""
    return {
        'CP': 'CONFIRMED',     # Confirmed Planet
        'KP': 'CONFIRMED',     # Known Planet
        'PC': 'CANDIDATE',     # Planet Candidate
        'APC': 'CANDIDATE',    # Ambiguous Planet Candidate
        'FP': 'FALSE POSITIVE', # False Positive
        'FA': 'FALSE POSITIVE'  # False Alarm
    }

def map_toi_to_standard_categories(toi_data):
    """MapeazÄƒ categoriile TOI la categoriile standardizate È™i numÄƒrÄƒ"""
    mapping = get_standard_category_mapping()
    
    if 'tfopwg_disp' in toi_data.columns:
        mapped_categories = toi_data['tfopwg_disp'].map(mapping)
        return mapped_categories.value_counts().to_dict()
    else:
        return {}

if __name__ == "__main__":
    demo_workflow()